{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ¾ Crop Prediction Model Training\n",
    "## Deep Learning Approach with Feature Engineering\n",
    "\n",
    "**Goal:** Train a neural network to predict the best crop based on soil and climate parameters\n",
    "\n",
    "**Input Features (7):**\n",
    "- N (Nitrogen)\n",
    "- P (Phosphorus)\n",
    "- K (Potassium)\n",
    "- Temperature\n",
    "- Humidity\n",
    "- pH\n",
    "- Rainfall\n",
    "\n",
    "**Output:** Crop recommendation with confidence scores\n",
    "\n",
    "**Dataset:** https://www.kaggle.com/datasets/atharvaingle/crop-recommendation-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# FIX: Protobuf compatibility for Kaggle\n# ============================================\nimport os\nos.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n\n# ============================================\n# Import Libraries\n# ============================================\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntry:\n    plt.style.use('seaborn-v0_8-darkgrid')\nexcept:\n    plt.style.use('seaborn-darkgrid')\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import (\n    classification_report, \n    confusion_matrix, \n    accuracy_score,\n    top_k_accuracy_score\n)\n\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, \n    ModelCheckpoint, \n    ReduceLROnPlateau\n)\nfrom tensorflow.keras.utils import to_categorical\n\n# Utilities\nimport joblib\nimport json\nfrom datetime import datetime\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Verify setup\nprint(f\"âœ… TensorFlow version: {tf.__version__}\")\nprint(f\"âœ… Pandas version: {pd.__version__}\")\nprint(f\"âœ… NumPy version: {np.__version__}\")\nprint(f\"âœ… GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n\nif len(tf.config.list_physical_devices('GPU')) > 0:\n    print(f\"âœ… Using GPU for training (faster)\")\nelse:\n    print(f\"âš ï¸  Using CPU for training (slower but reliable)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# For Kaggle: Use /kaggle/input/crop-recommendation-dataset/Crop_recommendation.csv\n",
    "# For local: Use your local path\n",
    "\n",
    "# Kaggle path (uncomment when running on Kaggle)\n",
    "df = pd.read_csv('/kaggle/input/crop-recommendation-dataset/Crop_recommendation.csv')\n",
    "\n",
    "# Local path (uncomment for local testing)\n",
    "# df = pd.read_csv('datasets/crop_data/Crop_recommendation.csv')\n",
    "\n",
    "print(f\"ğŸ“Š Dataset shape: {df.shape}\")\n",
    "print(f\"\\nğŸ“‹ First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"ğŸ“Š Dataset Information:\")\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"Total crops: {df['label'].nunique()}\")\n",
    "print(f\"\\nCrop distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Basic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"ğŸ” Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nğŸ“‹ Data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 3: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize crop distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "crop_counts = df['label'].value_counts()\n",
    "plt.bar(crop_counts.index, crop_counts.values, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Crop', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Distribution of Crops in Dataset', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Dataset is {'balanced' if crop_counts.std() < 10 else 'imbalanced'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    axes[i].hist(df[feature], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(f'{feature} Distribution', fontweight='bold')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[7])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df[features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 4: Feature Engineering\n",
    "\n",
    "Transform 7 input features into 15 engineered features for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features from raw inputs\n",
    "    7 features â†’ 15 features\n",
    "    \"\"\"\n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    # Original features\n",
    "    N = df_engineered['N']\n",
    "    P = df_engineered['P']\n",
    "    K = df_engineered['K']\n",
    "    temp = df_engineered['temperature']\n",
    "    humidity = df_engineered['humidity']\n",
    "    ph = df_engineered['ph']\n",
    "    rainfall = df_engineered['rainfall']\n",
    "    \n",
    "    # Engineered features\n",
    "    df_engineered['NPK_sum'] = N + P + K\n",
    "    df_engineered['N_P_ratio'] = N / (P + 0.01)\n",
    "    df_engineered['N_K_ratio'] = N / (K + 0.01)\n",
    "    df_engineered['P_K_ratio'] = P / (K + 0.01)\n",
    "    df_engineered['temp_humidity'] = temp * humidity / 100\n",
    "    df_engineered['rainfall_humidity'] = rainfall * humidity / 100\n",
    "    \n",
    "    # Categorical encoding\n",
    "    df_engineered['temp_category'] = pd.cut(temp, \n",
    "                                             bins=[-np.inf, 15, 25, 35, np.inf], \n",
    "                                             labels=[0, 1, 2, 3])\n",
    "    df_engineered['rainfall_category'] = pd.cut(rainfall, \n",
    "                                                 bins=[-np.inf, 100, 200, 300, np.inf], \n",
    "                                                 labels=[0, 1, 2, 3])\n",
    "    \n",
    "    return df_engineered\n",
    "\n",
    "# Apply feature engineering\n",
    "df_engineered = engineer_features(df)\n",
    "\n",
    "print(\"âœ… Feature engineering completed!\")\n",
    "print(f\"\\nOriginal features: 7\")\n",
    "print(f\"Engineered features: {len(df_engineered.columns) - 1}\")\n",
    "print(f\"\\nNew columns added:\")\n",
    "print([col for col in df_engineered.columns if col not in df.columns])\n",
    "\n",
    "df_engineered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 5: Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "feature_columns = [\n",
    "    'N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall',\n",
    "    'NPK_sum', 'N_P_ratio', 'N_K_ratio', 'P_K_ratio',\n",
    "    'temp_humidity', 'rainfall_humidity',\n",
    "    'temp_category', 'rainfall_category'\n",
    "]\n",
    "\n",
    "X = df_engineered[feature_columns].values\n",
    "y = df_engineered['label'].values\n",
    "\n",
    "print(f\"âœ… Features shape: {X.shape}\")\n",
    "print(f\"âœ… Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"âœ… Number of classes: {num_classes}\")\n",
    "print(f\"âœ… Crop names: {list(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 10% val, 10% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.1, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.111, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"âœ… Train samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"âœ… Validation samples: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"âœ… Test samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Features scaled (StandardScaler)\")\n",
    "print(f\"   Mean: {X_train_scaled.mean():.4f}\")\n",
    "print(f\"   Std: {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"âœ… Labels one-hot encoded\")\n",
    "print(f\"   Shape: {y_train_cat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Step 6: Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, num_classes):\n",
    "    \"\"\"\n",
    "    Build Deep Learning model for crop prediction\n",
    "    Architecture: 15 â†’ 256 â†’ 128 â†’ 64 â†’ num_classes\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Hidden layer 2\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_model(input_dim=15, num_classes=num_classes)\n",
    "\n",
    "print(\"âœ… Model built successfully!\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 7: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # Early stopping\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Model checkpoint\n",
    "    ModelCheckpoint(\n",
    "        'best_crop_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate on plateau\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ… Callbacks configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"ğŸ¯ Starting training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_cat,\n",
    "    validation_data=(X_val_scaled, y_val_cat),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 8: Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Top-3 Accuracy\n",
    "axes[1].plot(history.history['top_3_accuracy'], label='Train Top-3', linewidth=2)\n",
    "axes[1].plot(history.history['val_top_3_accuracy'], label='Val Top-3', linewidth=2)\n",
    "axes[1].set_title('Top-3 Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Top-3 Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[2].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[2].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[2].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 9: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy, test_top3_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_cat, verbose=0\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Top-3 Accuracy: {test_top3_accuracy*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nğŸ“Š Classification Report:\\n\")\n",
    "print(classification_report(\n",
    "    y_test, y_pred, \n",
    "    target_names=label_encoder.classes_,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Crop', fontsize=12)\n",
    "plt.ylabel('Actual Crop', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 10: Test with Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def predict_crop(features_dict, model, scaler, label_encoder):\n    \"\"\"\n    Predict crop from input features\n    \"\"\"\n    # Extract features\n    N = features_dict['nitrogen']\n    P = features_dict['phosphorus']\n    K = features_dict['potassium']\n    temp = features_dict['temperature']\n    humidity = features_dict['humidity']\n    ph = features_dict['ph_value']\n    rainfall = features_dict['rainfall']\n    \n    # Engineer features (MUST MATCH TRAINING!)\n    NPK_sum = N + P + K\n    N_P_ratio = N / (P + 0.01)\n    N_K_ratio = N / (K + 0.01)\n    P_K_ratio = P / (K + 0.01)\n    temp_humidity = temp * humidity / 100\n    rainfall_humidity = rainfall * humidity / 100\n    \n    # Categorical encoding (same bins as training)\n    if temp < 15:\n        temp_cat = 0\n    elif temp < 25:\n        temp_cat = 1\n    elif temp < 35:\n        temp_cat = 2\n    else:\n        temp_cat = 3\n    \n    if rainfall < 100:\n        rain_cat = 0\n    elif rainfall < 200:\n        rain_cat = 1\n    elif rainfall < 300:\n        rain_cat = 2\n    else:\n        rain_cat = 3\n    \n    # Create feature array (EXACT ORDER AS TRAINING)\n    features = np.array([[\n        N, P, K, temp, humidity, ph, rainfall,\n        NPK_sum, N_P_ratio, N_K_ratio, P_K_ratio,\n        temp_humidity, rainfall_humidity,\n        float(temp_cat), float(rain_cat)  # Convert to float\n    ]])\n    \n    # Scale features\n    X_scaled = scaler.transform(features)\n    \n    # Predict\n    proba = model.predict(X_scaled, verbose=0)[0]\n    \n    # Get top 3\n    top3_idx = np.argsort(proba)[-3:][::-1]\n    top3_crops = label_encoder.inverse_transform(top3_idx)\n    top3_conf = proba[top3_idx]\n    \n    return {\n        'predicted_crop': top3_crops[0].capitalize(),\n        'confidence': top3_conf[0] * 100,\n        'top_3': [(crop.capitalize(), conf * 100) for crop, conf in zip(top3_crops, top3_conf)]\n    }\n\n# Test cases\ntest_cases = [\n    {\n        'name': 'Rice',\n        'nitrogen': 90, 'phosphorus': 42, 'potassium': 43,\n        'temperature': 21, 'humidity': 82, 'ph_value': 6.5, 'rainfall': 203\n    },\n    {\n        'name': 'Wheat',\n        'nitrogen': 80, 'phosphorus': 40, 'potassium': 40,\n        'temperature': 18, 'humidity': 65, 'ph_value': 6.8, 'rainfall': 600\n    },\n    {\n        'name': 'Maize',\n        'nitrogen': 90, 'phosphorus': 40, 'potassium': 40,\n        'temperature': 21, 'humidity': 82, 'ph_value': 6.5, 'rainfall': 100\n    },\n    {\n        'name': 'Coffee',\n        'nitrogen': 100, 'phosphorus': 50, 'potassium': 35,\n        'temperature': 25, 'humidity': 85, 'ph_value': 6.8, 'rainfall': 200\n    },\n    {\n        'name': 'Cotton',\n        'nitrogen': 120, 'phosphorus': 50, 'potassium': 50,\n        'temperature': 25, 'humidity': 70, 'ph_value': 7.0, 'rainfall': 100\n    },\n    {\n        'name': 'Apple',\n        'nitrogen': 20, 'phosphorus': 125, 'potassium': 200,\n        'temperature': 22, 'humidity': 85, 'ph_value': 6.5, 'rainfall': 120\n    },\n    {\n        'name': 'Banana',\n        'nitrogen': 100, 'phosphorus': 80, 'potassium': 50,\n        'temperature': 28, 'humidity': 85, 'ph_value': 6.5, 'rainfall': 150\n    },\n    {\n        'name': 'Chickpea',\n        'nitrogen': 40, 'phosphorus': 60, 'potassium': 80,\n        'temperature': 25, 'humidity': 60, 'ph_value': 7.0, 'rainfall': 60\n    }\n]\n\nprint(\"ğŸ§ª Testing Sample Predictions:\\n\")\nprint(\"=\"*70)\n\ncorrect = 0\ntotal = len(test_cases)\n\nfor test in test_cases:\n    name = test.pop('name')\n    result = predict_crop(test, model, scaler, label_encoder)\n    \n    print(f\"\\nğŸŒ¾ Expected: {name}\")\n    print(f\"   Predicted: {result['predicted_crop']} ({result['confidence']:.1f}%)\")\n    print(f\"   Top-3:\")\n    for i, (crop, conf) in enumerate(result['top_3'], 1):\n        print(f\"      {i}. {crop:15s} {conf:5.1f}%\")\n    \n    if result['predicted_crop'].lower() == name.lower():\n        print(\"   âœ… CORRECT!\")\n        correct += 1\n    else:\n        top3_names = [c[0].lower() for c in result['top_3']]\n        if name.lower() in top3_names:\n            print(\"   âš ï¸  In Top-3 but not #1\")\n        else:\n            print(\"   âŒ Not in Top-3\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(f\"ğŸ“Š Test Results: {correct}/{total} correct ({correct/total*100:.1f}%)\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 11: Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('best_crop_model.h5')\n",
    "print(\"âœ… Saved: best_crop_model.h5\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"âœ… Saved: scaler.pkl\")\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "print(\"âœ… Saved: label_encoder.pkl\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_type': 'MLP',\n",
    "    'model_version': '1.0',\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'num_features': 15,\n",
    "    'num_classes': num_classes,\n",
    "    'feature_names': feature_columns,\n",
    "    'class_names': label_encoder.classes_.tolist(),\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_top3_accuracy': float(test_top3_accuracy),\n",
    "    'test_loss': float(test_loss),\n",
    "    'training_samples': len(X_train),\n",
    "    'validation_samples': len(X_val),\n",
    "    'test_samples': len(X_test),\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'best_val_accuracy': float(max(history.history['val_accuracy'])),\n",
    "    'architecture': {\n",
    "        'input': 15,\n",
    "        'dense_1': 256,\n",
    "        'dense_2': 128,\n",
    "        'dense_3': 64,\n",
    "        'output': num_classes\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"âœ… Saved: metadata.json\")\n",
    "\n",
    "print(\"\\nğŸ‰ All artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 12: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ CROP PREDICTION MODEL TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Final Results:\")\n",
    "print(f\"   Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"   Top-3 Accuracy: {test_top3_accuracy*100:.2f}%\")\n",
    "print(f\"   Total Crops: {num_classes}\")\n",
    "print(f\"   Total Parameters: {model.count_params():,}\")\n",
    "print(f\"   Training Epochs: {len(history.history['loss'])}\")\n",
    "print(f\"\\nğŸ“¦ Saved Files:\")\n",
    "print(f\"   - best_crop_model.h5 (Model)\")\n",
    "print(f\"   - scaler.pkl (Feature Scaler)\")\n",
    "print(f\"   - label_encoder.pkl (Label Encoder)\")\n",
    "print(f\"   - metadata.json (Training Info)\")\n",
    "print(f\"\\nğŸ¯ Next Steps:\")\n",
    "print(f\"   1. Download the model files\")\n",
    "print(f\"   2. Place in Django project: ml_models/\")\n",
    "print(f\"   3. Run: python manage.py runserver\")\n",
    "print(f\"   4. Test at: http://127.0.0.1:8000/predictions/crop/\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}